{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMjqT2LgtTUD"
      },
      "source": [
        "## Imports and download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkSmfdseZT7-",
        "outputId": "8862e050-7e41-4415-ec2c-ddd40a181313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tyring to install stuff\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.13.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Running as a Colab notebook\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"tyring to install stuff\")\n",
        "    ! pip install datasets transformers openai\n",
        "    print(\"Running as a Colab notebook\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNbLhXw4ZXht"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T75qU5PAZgYo",
        "outputId": "99d92ec5-a8ff-4845-97cb-03b8a54e96db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.30.2\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN0oWGd7f9pJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn import MSELoss, Linear, TransformerEncoderLayer, LayerNorm, TransformerEncoder\n",
        "from torch.optim import Adam\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import openai\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Check if CUDA is available and choose device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mQaswfQZ_o1"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"distilgpt2\"\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "from transformers import AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea-1zt3JaDdj",
        "outputId": "611fca38-a718-48f2-be3b-9c5cba9e9ed5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdK8501maTZl",
        "outputId": "f6f34a22-2087-4145-8655-d979927a3168"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2Block(\n",
              "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (attn): GPT2Attention(\n",
              "    (c_attn): Conv1D()\n",
              "    (c_proj): Conv1D()\n",
              "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (mlp): GPT2MLP(\n",
              "    (c_fc): Conv1D()\n",
              "    (c_proj): Conv1D()\n",
              "    (act): NewGELUActivation()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.transformer.h[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud1Ea0Ntb1oK",
        "outputId": "b57dc1f2-c23a-450c-9c7d-a84acea27f38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def is_toeplitz(matrix):\n",
        "    rows = len(matrix)\n",
        "    cols = len(matrix[0])\n",
        "\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            if i > 0 and j > 0 and matrix[i][j] != matrix[i-1][j-1]:\n",
        "                return False\n",
        "\n",
        "    return True\n",
        "is_toeplitz(model.transformer.h[1].mlp.c_proj.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2StFnR7aJB0",
        "outputId": "542b18f8-a319-45b1-b9b9-a775cf21dcc6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2MLP(\n",
              "  (c_fc): Conv1D()\n",
              "  (c_proj): Conv1D()\n",
              "  (act): NewGELUActivation()\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.transformer.h[1].mlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1sWxpx5tYVp"
      },
      "source": [
        "## Hook into model and optimize sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF4hALDAuEZO"
      },
      "outputs": [],
      "source": [
        "def unembed_and_decode(embeds_input):\n",
        "  \"\"\"\n",
        "  Given an embedding vector, decode each token by using the transpose of the embedding matrix\n",
        "  and grabbing the vocab token with the highest probability on each token.\n",
        "\n",
        "  Also do this with the unembedding matrix as well.\n",
        "  \"\"\"\n",
        "  with torch.no_grad():\n",
        "      # Get the pre-trained embeddings\n",
        "      pretrained_embeddings = model.transformer.wte.weight\n",
        "      # Calculate dot product between input embeddings and pre-trained embeddings\n",
        "      dot_product = torch.matmul(embeds_input, pretrained_embeddings.t())\n",
        "\n",
        "      # Get the index of the highest value along dimension 2 (tokens)\n",
        "      _, tokens = torch.max(dot_product, dim=2)\n",
        "\n",
        "  # Decode tokens into text using the tokenizer\n",
        "  text = tokenizer.batch_decode(tokens.tolist(), skip_special_tokens=True)\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zxln1LMKRM-"
      },
      "outputs": [],
      "source": [
        "def optimize_for_neuron(starting_sentence, layer_num=1, neuron_index=0, mlp_or_attention=\"mlp\"):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    neuron_indices: List of indices.\n",
        "    mlp_or_attention (str): 'mlp' or 'attention'\n",
        "  \"\"\"\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n",
        "  inputs = tokenizer(starting_sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  # Get embeddings\n",
        "  with torch.no_grad():\n",
        "      embeddings = model.transformer.wte(inputs[\"input_ids\"])\n",
        "\n",
        "  # Make embeddings require gradient\n",
        "  embeddings.requires_grad_(True)\n",
        "\n",
        "  # Create an optimizer for the embeddings\n",
        "  optimizer = AdamW([embeddings], lr=0.1)  # You may need to adjust the learning rate\n",
        "  pre_embeddings = embeddings.detach().clone()\n",
        "  print(embeddings)\n",
        "  print(unembed_and_decode(pre_embeddings))\n",
        "  len_example = embeddings.shape[1] - 1\n",
        "\n",
        "  if 'mlp' in mlp_or_attention:\n",
        "    layer = model.transformer.h[layer_num].mlp\n",
        "  else:\n",
        "    raise NotImplementedError(\"Haven't implemented attention block yet\")\n",
        "  activation_saved = [torch.tensor(0.0)]\n",
        "  def hook(model, input, output):\n",
        "    # The output is a tensor. You can index it to get the activation of a specific neuron.\n",
        "    # Here we're getting the activation of the 0th neuron.\n",
        "    # TODO: Figure out what neruon this is actually grabbing. Why is it\n",
        "    activation = output[0, len_example, neuron_index]\n",
        "    activation_saved[0] = activation\n",
        "  handle = layer.register_forward_hook(hook)\n",
        "\n",
        "  losses = []\n",
        "  dist = 0.0\n",
        "  for i in tqdm(range(100)):\n",
        "    outputs = model(inputs_embeds=embeddings, attention_mask=inputs.attention_mask)\n",
        "    loss = -torch.sigmoid(activation_saved[0])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    dist = torch.sum(embeddings - pre_embeddings).item()\n",
        "    losses.append(loss)\n",
        "    if i % 25 == 0:\n",
        "      tqdm.write(f\"\\n{dist} and then {loss}\\n\")\n",
        "      tqdm.write(unembed_and_decode(embeddings)[0])\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8CMS1fpLg2Z"
      },
      "outputs": [],
      "source": [
        "# input_sentence_1 = \"In the midst of a vibrant summer morning, with the sun casting its golden rays upon the lush green meadows and the fragrant wildflowers swaying gently in the warm breeze, a multitude of birds chirped melodiously while gracefully soaring across the clear blue sky, their wings glimmering like tiny diamonds as they embraced the boundless freedom of the open air, and nearby, a majestic oak tree stood tall and proud, its branches extending outward in a magnificent display of nature's artistry, providing shade and shelter for a variety of creatures that sought solace beneath its protective canopy, including a family of squirrels playfully darting between the branches, their bushy tails serving as vibrant accents against the backdrop of verdant leaves, and as the day progressed, the distant rumble of thunder gradually grew louder, heralding the imminent arrival of a summer storm, as dark clouds gathered overhead, casting an ephemeral gloom over the once vibrant landscape, yet even in the face of this impending tempest, there was an undeniable beauty in the contrast between the electric flashes of lightning that briefly illuminated the sky and the cascading raindrops that danced upon the earth, breathing life into the thirsty soil and rejuvenating the flora and fauna, and as the storm subsided, a mesmerizing rainbow emerged, arching gracefully across the horizon, its vibrant hues painting a breathtaking scene that filled hearts with awe and wonder, reminding us of the ever-present magic and resilience of nature, and in that fleeting moment, as the world basked in the afterglow of the storm, a profound sense of gratitude and harmony washed over everything, reminding us of our intricate connection to the vast tapestry of existence.\"\n",
        "# input_sentence_2 = \"The fundamental principles of calculus provide a powerful framework for understanding and analyzing the rates of change and accumulation of quantities in various fields of mathematics and science, enabling us to model and solve complex real-world problems with precision and rigor.\"\n",
        "# input_sentence_3 = \"I'm sorry for the misunderstanding, but as an AI developed by OpenAI, I don't have direct access to individual sentences or documents from my training data. I was trained on a mixture of licensed data, data created by human trainers, and publicly available data. These sources may contain a wide range of data, including books, websites, and other texts, so I don't have the ability to recall or generate any specific sentence from the training data. I generate responses based on patterns and information in the data I was trained on.\"\n",
        "# losses = optimize_for_neuron(input_sentence_3, neuron_index=2, layer_num=5)\n",
        "# # Plot losses\n",
        "# plt.figure(figsize=(10,6))\n",
        "# plt.plot([loss.cpu().detach() for loss in losses])\n",
        "# plt.title('Loss curve')\n",
        "# plt.xlabel('Iteration')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMygm2yzt5Pz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def optimize_for_neuron_whole_input(neuron_index=0, layer_num=1, mlp_or_attention=\"mlp\", num_tokens=10, num_iterations=200):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      neuron_indices: List of indices.\n",
        "      mlp_or_attention (str): 'mlp' or 'attention'\n",
        "    \"\"\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n",
        "\n",
        "    # Start with random embeddings\n",
        "    embeddings = torch.randn((1, num_tokens, model.config.n_embd), device=device, requires_grad=True)\n",
        "\n",
        "    # Create an optimizer for the embeddings\n",
        "    optimizer = AdamW([embeddings], lr=0.1)  # You may need to adjust the learning rate\n",
        "\n",
        "    if 'mlp' in mlp_or_attention:\n",
        "        layer = model.transformer.h[layer_num].mlp\n",
        "    else:\n",
        "        raise NotImplementedError(\"Haven't implemented attention block yet\")\n",
        "\n",
        "    activation_saved = [torch.tensor(0.0, device=device)]\n",
        "    def hook(model, input, output):\n",
        "        # The output is a tensor. We're getting the average activation of the neuron across all tokens.\n",
        "        activation = output[0, :, neuron_index].mean()\n",
        "        activation_saved[0] = activation\n",
        "    handle = layer.register_forward_hook(hook)\n",
        "\n",
        "    losses = []\n",
        "    for i in tqdm(range(num_iterations), position=0, leave=True):\n",
        "        # Construct input for the model using the embeddings directly\n",
        "        outputs = model(inputs_embeds=embeddings)\n",
        "        # We want to maximize activation, which is equivalent to minimizing negative activation\n",
        "        loss = -torch.sigmoid(activation_saved[0])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "        if i % (num_iterations//30) == 0:\n",
        "            tqdm.write(f\"Loss at step {i}: {loss.item()}\\n\", end='')\n",
        "            tqdm.write(unembed_and_decode(embeddings)[0], end='')\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    handle.remove()  # Don't forget to remove the hook!\n",
        "    return losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scRH0RiR28VY"
      },
      "outputs": [],
      "source": [
        "# losses = optimize_for_neuron_whole_input(neuron_index=2, layer_num=5, num_tokens=20)\n",
        "# # Plot losses\n",
        "# plt.figure(figsize=(10,6))\n",
        "# plt.plot(losses)\n",
        "# plt.title('Loss curve')\n",
        "# plt.xlabel('Iteration')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.grid(True)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5AHLWhI29a2"
      },
      "outputs": [],
      "source": [
        "class TransformerAutoencoder(torch.nn.Module):\n",
        "    def __init__(self, model_checkpoint, latent_dim=100, nhead=2, num_layers=6):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load the pretrained model\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
        "\n",
        "        # Create the encoder\n",
        "        self.encoder = copy.deepcopy(base_model)\n",
        "        self.encoder.lm_head = Linear(base_model.config.n_embd, latent_dim)\n",
        "\n",
        "        # Create the decoder from scratch\n",
        "        self.projection = Linear(latent_dim, base_model.config.n_embd)\n",
        "        encoder_layer = TransformerEncoderLayer(d_model=base_model.config.n_embd, nhead=nhead)\n",
        "        encoder_norm = LayerNorm(base_model.config.n_embd)\n",
        "        self.decoder = TransformerEncoder(encoder_layer, num_layers=num_layers, norm=encoder_norm)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Encode the input\n",
        "        latent = self.encoder(input_ids, attention_mask=attention_mask).logits\n",
        "\n",
        "        # Project the latent representation to the original embedding dimension\n",
        "        projected = self.projection(latent)\n",
        "\n",
        "        # Decode the projected representation\n",
        "        reconstructed_embeddings = self.decoder(projected)\n",
        "\n",
        "        return reconstructed_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2uMvZaTF7M-"
      },
      "outputs": [],
      "source": [
        "class Gpt2Autoencoder(torch.nn.Module):\n",
        "    def __init__(self, model_checkpoint, latent_dim=10, nhead=2, num_layers=6):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load the pretrained model\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
        "\n",
        "        # Create the encoder\n",
        "        self.encoder = copy.deepcopy(base_model)\n",
        "        self.encoder.lm_head = Linear(base_model.config.n_embd, latent_dim)\n",
        "\n",
        "        # Create the decoder from scratch\n",
        "        self.projection = Linear(latent_dim, base_model.config.n_embd)\n",
        "\n",
        "        self.decoder = TransformerEncoder(encoder_layer, num_layers=num_layers, norm=encoder_norm)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Encode the input\n",
        "        latent = self.encoder(input_ids, attention_mask=attention_mask).logits\n",
        "\n",
        "        # Project the latent representation to the original embedding dimension\n",
        "        projected = self.projection(latent)\n",
        "\n",
        "        # Decode the projected representation\n",
        "        reconstructed_embeddings = self.decoder(projected)\n",
        "\n",
        "        return reconstructed_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llYWpTHM3Aol"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(model, tokenizer, max_length=50):\n",
        "    # Randomly select a token\n",
        "    random_token = random.randint(0, 50256)\n",
        "\n",
        "    # Convert the token to a tensor\n",
        "    input_ids = torch.tensor([[random_token]]).to(device)\n",
        "\n",
        "    # Generate a sequence of tokens\n",
        "    output = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=1.0, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    # Decode the tokens into a sentence\n",
        "    sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return sentence\n",
        "\n",
        "def train_autoencoder(autoencoder, criterion, optimizer, base_model, tokenizer, num_epochs=1000, print_every=50, save_path=\"transformer-autoencoder.pt\", load_path=None):\n",
        "    scaler = GradScaler()  # Enables mixed-precision training\n",
        "    loss_values = []\n",
        "\n",
        "    # Load the model's parameters from a checkpoint if provided\n",
        "    if load_path is not None:\n",
        "        autoencoder.load_state_dict(torch.load(load_path))\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        # Generate a sentence with the pretrained model\n",
        "        input_sentence = generate_sentence(base_model, tokenizer, max_length=50)\n",
        "\n",
        "        # Prepare the inputs for the autoencoder\n",
        "        input_ids = tokenizer.encode(input_sentence, return_tensors=\"pt\").to(device)\n",
        "        original_embeddings = base_model.transformer.wte(input_ids)\n",
        "\n",
        "        # Run the autoencoder and compute the loss\n",
        "        with autocast():\n",
        "            reconstructed_embeddings = autoencoder(input_ids)\n",
        "            loss = criterion(reconstructed_embeddings, original_embeddings)\n",
        "\n",
        "        # Backpropagation\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Record the loss value for plotting\n",
        "        loss_values.append(loss.item())\n",
        "\n",
        "        # Print progress and save model every 'print_every' epochs\n",
        "        if epoch % print_every == 0:\n",
        "            print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item()}\")\n",
        "            torch.save(autoencoder.state_dict(), save_path)\n",
        "\n",
        "    # Plot the loss values\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(loss_values)\n",
        "    plt.title('Loss Curve')\n",
        "    plt.xlabel('Training Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfN8JUtdDrea"
      },
      "outputs": [],
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
        "\n",
        "# Initialize autoencoder\n",
        "autoencoder = TransformerAutoencoder('distilgpt2').to(device)\n",
        "\n",
        "# Initialize loss function, optimizer, and gradient scaler for mixed-precision training\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(autoencoder.parameters())\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Call the training function without a path to a checkpoint\n",
        "# train_autoencoder(autoencoder, criterion, optimizer, model, tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbVtiwjwKbOL",
        "outputId": "16bc27c6-78ca-4b98-be3b-1c3af5e48a46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "autoencoder = TransformerAutoencoder('distilgpt2').to(device)\n",
        "autoencoder.load_state_dict(torch.load('transformer-autoencoder.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkwYgmQIS_DK",
        "outputId": "691656a9-8117-4203-8617-0a0d0a85b56c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.9260, -1.3985,  0.9502,  ..., -0.9059,  0.5368,  0.5839],\n",
              "         [-1.0884, -1.3361,  0.5918,  ..., -0.7432,  0.4180,  0.9456],\n",
              "         [-0.9033, -1.3022,  0.8088,  ..., -0.8211,  0.2759,  0.7417],\n",
              "         [-0.8096, -1.5392,  1.0889,  ..., -0.9813,  0.6752,  0.7932]]],\n",
              "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Encode the sentence\n",
        "input_ids = tokenizer.encode(\"I am a banana\", return_tensors=\"pt\").to(device)\n",
        "original_embeddings = model.transformer.wte(input_ids)\n",
        "autoencoder(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-r7wrKH9i7B",
        "outputId": "4c7d7fe5-f172-4e6f-ae4b-e04cd745a68f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original sentence: uch the people of Israel would be able to have their way around what is perceived as a state and, ultimately, an absolute monarchy - a dictatorship not just to the one which has created the state. We saw this at the founding of the State of\n",
            "Reconstructed sentence: [' horizont horizont horizont horizont horizont horizont horizont point horizont horizont horizont horizont horizont horizont horizont horizont point horizont horizont horizont horizontS point horizont point horizont horizont suspic point horizont point horizont suspic horizont horizont horizont suspic horizont horizont horizont point horizont horizont horizont horizont point horizont horizont point point']\n",
            "Original sentence:  among it does the same thing in China. The only difference in the situation is that it has more people in China than the people in the United States, but in China the number increases so much. As a consequence for this, China is quite hard\n",
            "Reconstructed sentence: [' suspic horizont horizont horizont mathemat horizont horizont horizont horizont horizont horizont horizont horizont horizontSSS pointS horizont horizont pointS horizont horizont horizont point horizont point point horizontS pointS pointS point point suspic point point pointSSSSSS point point']\n",
            "Original sentence: neck I'd say that it's nice to have a little conversation about what happens in these circumstances. The whole season might change as I type, but I also love having to think about certain aspects of the dynamics in our lives but also on the things\n",
            "Reconstructed sentence: [' horizont horizont horizont suspic horizont horizont horizont point horizont mathemat point horizont suspic horizont horizont horizont horizont horizont point pointSS point horizont horizont horizont horizont horizont horizont horizont horizont horizont horizont horizont point horizont horizont suspic horizont horizont horizont point point horizont point point suspic horizont horizont horizont']\n",
            "Original sentence:  InitiativeDATE: Nov. 24, 2007\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Reconstructed sentence: [' horizont horizont horizont horizont horizont horizont horizont suspic pointSSSSSSSSSSSSSSSS pointSSSS']\n",
            "Original sentence:  unsatisfSellin is no longer a company, and it would be difficult to predict who will get it.\n",
            "Reconstructed sentence: [' horizont horizont horizont horizont horizont horizont point horizont horizont horizont horizont suspic horizont horizont point horizont horizont suspic horizont horizont mathemat point']\n",
            "Original sentence:  AdThe first in the series, ‪Vadam‏ (Aurail Singh, a former teacher and a political activist, has won the Nobel Peace Prize.)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "‪Vadam – Vadam\n",
            "Reconstructed sentence: [' horizont horizont horizont point suspic horizont horizont of horizont horizont point horizont point horizont horizont pointSS point suspic horizont point point horizont suspic horizont horizont horizont horizont horizont horizont horizont horizont horizontSSSSS pointSS horizontS point pointS point']\n",
            "Original sentence: paid.gov?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Reconstructed sentence: [' horizont mathemat mathemat horizontSSSSSSSSSSSSS viewSSSSSSSSS']\n",
            "Original sentence:  purchaseThe American Bar-Lecture Society, as well as the American Petroleum Institute, is making a commitment to create a “†††††*†††††††††††††††††\n",
            "Reconstructed sentence: [' horizont horizont point suspic point point horizont horizont suspic horizont pointS point horizontS horizontS mathemat horizont horizont horizont horizont horizont horizont horizont horizont point pointS point pointS point point point point pointS point pointSS pointS pointSS pointSS']\n",
            "Original sentence: ches The only person in the world without a job or training is a woman. There has been a great deal of research and experience on the topic. From the beginning I was one of the first of you to hear what people are saying, and why\n",
            "Reconstructed sentence: [' horizont horizont horizont point horizont horizont mathemat horizont horizont suspic horizont horizont horizont horizont horizont point pointS point point horizont point mathemat horizont point horizont horizont horizontSSS pointS point point pointS pointSSS point point horizont horizont point horizont point horizont horizont']\n",
            "Original sentence:  uptick to reduce the number of people infected during the 2015-16 school year.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Reconstructed sentence: [' horizont horizont horizont horizont point point horizont horizont horizont suspic horizont horizontSS point point viewSSSS']\n",
            "Average sentence similarity between original and reconstructed sentences: 0.7002239890339217\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def get_sentence_similarity(sentence1, sentence2):\n",
        "    # Get embeddings for both sentences\n",
        "    response1 = openai.Embedding.create(input=sentence1, model=\"text-embedding-ada-002\")\n",
        "    response2 = openai.Embedding.create(input=sentence2, model=\"text-embedding-ada-002\")\n",
        "\n",
        "    embedding1 = np.array(response1['data'][0]['embedding'])\n",
        "    embedding2 = np.array(response2['data'][0]['embedding'])\n",
        "\n",
        "    # Compute cosine similarity between embeddings\n",
        "    similarity = cosine_similarity(embedding1.reshape(1, -1), embedding2.reshape(1, -1))\n",
        "\n",
        "    return similarity[0][0]\n",
        "\n",
        "def evaluate_autoencoder(autoencoder, base_model, tokenizer, num_samples=10):\n",
        "    # Prepare the autoencoder for evaluation\n",
        "    # autoencoder.eval() # <- do we need to implement this?\n",
        "\n",
        "    similarities = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Generate a sentence with the base model\n",
        "        input_sentence = generate_sentence(base_model, tokenizer, max_length=50)\n",
        "        print(f\"Original sentence: {input_sentence}\")\n",
        "\n",
        "        # Encode the sentence\n",
        "        input_ids = tokenizer.encode(input_sentence, return_tensors=\"pt\").to(device)\n",
        "        original_embeddings = base_model.transformer.wte(input_ids)\n",
        "\n",
        "        # Pass the encoded sentence through the autoencoder\n",
        "        with torch.no_grad():\n",
        "            reconstructed_embeddings = autoencoder(input_ids)\n",
        "\n",
        "        # Decode the output of the autoencoder\n",
        "        reconstructed_sentence = unembed_and_decode(reconstructed_embeddings)\n",
        "        print(f\"Reconstructed sentence: {reconstructed_sentence}\")\n",
        "\n",
        "        # Compute the sentence similarity between the original and reconstructed sentences\n",
        "        similarity = get_sentence_similarity(input_sentence, reconstructed_sentence)\n",
        "        similarities.append(similarity)\n",
        "\n",
        "    # Compute the average sentence similarity\n",
        "    average_similarity = np.mean(similarities)\n",
        "\n",
        "    print(f\"Average sentence similarity between original and reconstructed sentences: {average_similarity}\")\n",
        "\n",
        "# Call the evaluation function\n",
        "evaluate_autoencoder(autoencoder, model, tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6SK81sxNI8V"
      },
      "outputs": [],
      "source": [
        "def train_autoencoder(autoencoder, criterion, optimizer, base_model, tokenizer, num_epochs=1000, print_every=50, save_path=\"transformer-autoencoder.pt\", load_path=None):\n",
        "    scaler = GradScaler()  # Enables mixed-precision training\n",
        "    loss_values = []\n",
        "    similarity_values = []\n",
        "\n",
        "    # Load the model's parameters from a checkpoint if provided\n",
        "    if load_path is not None:\n",
        "        autoencoder.load_state_dict(torch.load(load_path))\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        # Generate a sentence with the pretrained model\n",
        "        input_sentence = generate_sentence(base_model, tokenizer, max_length=50)\n",
        "\n",
        "        # Prepare the inputs for the autoencoder\n",
        "        input_ids = tokenizer.encode(input_sentence, return_tensors=\"pt\").to(device)\n",
        "        original_embeddings = base_model.transformer.wte(input_ids)\n",
        "\n",
        "        # Run the autoencoder and compute the loss\n",
        "        with autocast():\n",
        "            reconstructed_embeddings = autoencoder(input_ids)\n",
        "            loss = criterion(reconstructed_embeddings, original_embeddings)\n",
        "\n",
        "        # Backpropagation\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Record the loss value for plotting\n",
        "        loss_values.append(loss.item())\n",
        "\n",
        "        # Compute the sentence similarity between the original and reconstructed sentences\n",
        "        reconstructed_sentence = unembed_and_decode(reconstructed_embeddings)\n",
        "        similarity = get_sentence_similarity(input_sentence, reconstructed_sentence)\n",
        "        similarity_values.append(similarity)\n",
        "\n",
        "        # Print progress and save model every 'print_every' epochs\n",
        "        if epoch % print_every == 0:\n",
        "            print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item()}, Similarity: {similarity}\")\n",
        "            torch.save(autoencoder.state_dict(), save_path)\n",
        "\n",
        "    # Plot the loss values and similarities\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Training Step')\n",
        "    ax1.set_ylabel('Loss', color=color)\n",
        "    ax1.plot(loss_values, color=color)\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Similarity', color=color)  # we already handled the x-label with ax1\n",
        "    ax2.plot(similarity_values, color=color)\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "K7OoLj0kWZdH",
        "outputId": "af237cf2-d7c8-4845-eaab-8f444da6411e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/1000, Loss: 0.014136187732219696, Similarity: 0.7101909368074204\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 32/1000 [00:43<21:43,  1.35s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-dd0171541da2>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'transformer-autoencoder.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-126-fb05ef17e16f>\u001b[0m in \u001b[0;36mtrain_autoencoder\u001b[0;34m(autoencoder, criterion, optimizer, base_model, tokenizer, num_epochs, print_every, save_path, load_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Compute the sentence similarity between the original and reconstructed sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mreconstructed_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munembed_and_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentence_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructed_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0msimilarity_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-83cadfb984cd>\u001b[0m in \u001b[0;36mget_sentence_similarity\u001b[0;34m(sentence1, sentence2)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Get embeddings for both sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mresponse1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-embedding-ada-002\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mresponse2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-embedding-ada-002\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0membedding1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/embedding.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m# If a user specifies base64, we'll just return the encoded string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     ) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], bool, str]:\n\u001b[0;32m--> 288\u001b[0;31m         result = self.request_raw(\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0m_thread_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_create_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             result = _thread_context.session.request(\n\u001b[0m\u001b[1;32m    597\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0mabs_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    527\u001b[0m         }\n\u001b[1;32m    528\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    441\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    715\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    464\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1272\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
        "\n",
        "# Initialize autoencoder\n",
        "autoencoder = TransformerAutoencoder('distilgpt2').to(device)\n",
        "\n",
        "# Initialize loss function, optimizer, and gradient scaler for mixed-precision training\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(autoencoder.parameters())\n",
        "scaler = GradScaler()\n",
        "\n",
        "\n",
        "train_autoencoder(autoencoder, criterion, optimizer, model, tokenizer, load_path='transformer-autoencoder.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDnci_DnWaOV"
      },
      "outputs": [],
      "source": [
        "def optimize_for_neuron_whole_input(neuron_index=0, layer_num=1, mlp_or_attention=\"mlp\", num_tokens=10, num_iterations=200):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      neuron_indices: List of indices.\n",
        "      mlp_or_attention (str): 'mlp' or 'attention'\n",
        "    \"\"\"\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n",
        "    autoencoder = TransformerAutoencoder('distilgpt2').to(device)\n",
        "    autoencoder.load_state_dict(torch.load('transformer-autoencoder.pt'))\n",
        "\n",
        "    # Get the dimensionality of the latent space\n",
        "    latent_dim = autoencoder.encoder.lm_head.weight.shape[0]\n",
        "\n",
        "    # Start with random latent vectors\n",
        "    latent_vectors = torch.randn((1, num_tokens, latent_dim), device=device, requires_grad=True)\n",
        "\n",
        "    # Create an optimizer for the latent vectors\n",
        "    optimizer = AdamW([latent_vectors], lr=0.1)  # You may need to adjust the learning rate\n",
        "\n",
        "    if 'mlp' in mlp_or_attention:\n",
        "        layer = autoencoder.decoder.transformer.h[layer_num].mlp\n",
        "    else:\n",
        "        raise NotImplementedError(\"Haven't implemented attention block yet\")\n",
        "\n",
        "    activation_saved = [torch.tensor(0.0, device=device)]\n",
        "    def hook(model, input, output):\n",
        "        # The output is a tensor. We're getting the average activation of the neuron across all tokens.\n",
        "        activation = output[0, :, neuron_index].mean()\n",
        "        activation_saved[0] = activation\n",
        "    handle = layer.register_forward_hook(hook)\n",
        "\n",
        "    losses = []\n",
        "    for i in tqdm(range(num_iterations), position=0, leave=True):\n",
        "        # Construct input for the model using the embeddings directly\n",
        "        embeddings = autoencoder.decoder(latent_vectors)\n",
        "        outputs = base_model(inputs_embeds=embeddings)\n",
        "        # We want to maximize activation, which is equivalent to minimizing negative activation\n",
        "        loss = -torch.sigmoid(activation_saved[0])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "        if i % (num_iterations//30) == 0:\n",
        "            tqdm.write(f\"Loss at step {i}: {loss.item()}\\n\", end='')\n",
        "            tqdm.write(unembed_and_decode(embeddings, tokenizer, base_model)[0], end='')\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    handle.remove()  # Don't forget to remove the hook!\n",
        "    return losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "z5kPcstJf9j4",
        "outputId": "0e98500a-e005-4a92-f46b-1ca56196e2c4"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-137-c92dec1fb77d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimize_for_neuron_whole_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-136-245de21a9af3>\u001b[0m in \u001b[0;36moptimize_for_neuron_whole_input\u001b[0;34m(neuron_index, layer_num, mlp_or_attention, num_tokens, num_iterations)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'mlp'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmlp_or_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Haven't implemented attention block yet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1615\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TransformerEncoder' object has no attribute 'transformer'"
          ]
        }
      ],
      "source": [
        "optimize_for_neuron_whole_input()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IwO9VJvgNDT",
        "outputId": "8eab2846-8985-4eb3-85e8-181998c4530b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 768])"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak3HdRe7g_ke"
      },
      "outputs": [],
      "source": [
        "0"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
