{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scottviteri/NNVisualizationWithAutoencoder/blob/main/Deep_dream_for_llms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMjqT2LgtTUD"
      },
      "source": [
        "## Imports and download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkSmfdseZT7-",
        "outputId": "cf1d8bbe-c4e8-4bd0-e8b6-442cc7ea17c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tyring to install stuff\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.13.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: weightwatcher in /usr/local/lib/python3.10/dist-packages (0.7.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from weightwatcher) (3.7.1)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from weightwatcher) (0.1.6)\n",
            "Requirement already satisfied: powerlaw in /usr/local/lib/python3.10/dist-packages (from weightwatcher) (1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from weightwatcher) (1.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->weightwatcher) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->weightwatcher) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->weightwatcher) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->weightwatcher) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->weightwatcher) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->weightwatcher) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->weightwatcher) (2.8.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from matplotlib-inline->weightwatcher) (5.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from powerlaw->weightwatcher) (1.10.1)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.10/dist-packages (from powerlaw->weightwatcher) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->weightwatcher) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->weightwatcher) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->weightwatcher) (1.16.0)\n",
            "Running as a Colab notebook\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"tyring to install stuff\")\n",
        "    ! pip install datasets transformers openai weightwatcher\n",
        "    print(\"Running as a Colab notebook\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    # from IPython import get_ipython\n",
        "\n",
        "    # ipython = get_ipython()\n",
        "    # # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    # ipython.magic(\"load_ext autoreload\")\n",
        "    # ipython.magic(\"autoreload 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eNbLhXw4ZXht"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yN0oWGd7f9pJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "541302f7-f207-4ef4-e470-9a037419928a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "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\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn import MSELoss, Linear, TransformerEncoderLayer, LayerNorm, TransformerEncoder\n",
        "from torch.optim import Adam\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import copy\n",
        "#from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import openai\n",
        "import numpy as np\n",
        "import random\n",
        "from torch import nn\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "import transformers\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm.auto import tqdm\n",
        "import weightwatcher as ww\n",
        "import code\n",
        "\n",
        "openai.api_key = input()\n",
        "# Check if CUDA is available and choose device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea-1zt3JaDdj",
        "outputId": "ee07a63a-82d2-4bc5-e778-7abaafcbbdf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1sWxpx5tYVp"
      },
      "source": [
        "## Hook into model and optimize sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LF4hALDAuEZO"
      },
      "outputs": [],
      "source": [
        "def unembed_and_decode(embeds_input):\n",
        "  \"\"\"\n",
        "  Given an embedding vector, decode each token by using the transpose of the embedding matrix\n",
        "  and grabbing the vocab token with the highest probability on each token.\n",
        "\n",
        "  Also do this with the unembedding matrix as well.\n",
        "  \"\"\"\n",
        "  with torch.no_grad():\n",
        "      with autocast():\n",
        "        # Get the pre-trained embeddings\n",
        "        pretrained_embeddings = model.transformer.wte.weight\n",
        "        # if pretrained_embeddings.dtype != embeds_input.dtype:\n",
        "          # These types don't match so we use auto cast.\n",
        "        #   print(f\"types don't match, got for embeds inputs { embeds_input.dtype}, and {pretrained_embeddings.dtype} for embeddings matrix from gpt2 model\")\n",
        "        # Calculate dot product between input embeddings and pre-trained embeddings\n",
        "        dot_product = torch.matmul(embeds_input, pretrained_embeddings.t())\n",
        "\n",
        "        # Get the index of the highest value along dimension 2 (tokens)\n",
        "        _, tokens = torch.max(dot_product, dim=2)\n",
        "\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  # Decode tokens into text using the tokenizer\n",
        "  text = tokenizer.batch_decode(tokens.tolist(), skip_special_tokens=True, max_length=model.config.n_ctx, padding=\"max_length\")\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0zxln1LMKRM-"
      },
      "outputs": [],
      "source": [
        "def optimize_for_neuron(starting_sentence, layer_num=1, neuron_index=0, mlp_or_attention=\"mlp\"):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    neuron_indices: List of indices.\n",
        "    mlp_or_attention (str): 'mlp' or 'attention'\n",
        "  \"\"\"\n",
        "  model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n",
        "  inputs = tokenizer(starting_sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  # Get embeddings\n",
        "  with torch.no_grad():\n",
        "      embeddings = model.transformer.wte(inputs[\"input_ids\"])\n",
        "\n",
        "  # Make embeddings require gradient\n",
        "  embeddings.requires_grad_(True)\n",
        "\n",
        "  # Create an optimizer for the embeddings\n",
        "  optimizer = AdamW([embeddings], lr=0.1)  # You may need to adjust the learning rate\n",
        "  pre_embeddings = embeddings.detach().clone()\n",
        "  print(embeddings)\n",
        "  print(unembed_and_decode(pre_embeddings))\n",
        "  len_example = embeddings.shape[1] - 1\n",
        "\n",
        "  if 'mlp' in mlp_or_attention:\n",
        "    layer = model.transformer.h[layer_num].mlp\n",
        "  else:\n",
        "    raise NotImplementedError(\"Haven't implemented attention block yet\")\n",
        "  activation_saved = [torch.tensor(0.0)]\n",
        "  def hook(model, input, output):\n",
        "    # The output is a tensor. You can index it to get the activation of a specific neuron.\n",
        "    # Here we're getting the activation of the 0th neuron.\n",
        "    # TODO: Figure out what neruon this is actually grabbing. Why is it\n",
        "    activation = output[0, len_example, neuron_index]\n",
        "    activation_saved[0] = activation\n",
        "  handle = layer.register_forward_hook(hook)\n",
        "\n",
        "  losses = []\n",
        "  dist = 0.0\n",
        "  for i in tqdm(range(100)):\n",
        "    outputs = model(inputs_embeds=embeddings, attention_mask=inputs.attention_mask)\n",
        "    loss = -torch.sigmoid(activation_saved[0])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    dist = torch.sum(embeddings - pre_embeddings).item()\n",
        "    losses.append(loss)\n",
        "    if i % 25 == 0:\n",
        "      tqdm.write(f\"\\n{dist} and then {loss}\\n\")\n",
        "      tqdm.write(unembed_and_decode(embeddings)[0])\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W8CMS1fpLg2Z"
      },
      "outputs": [],
      "source": [
        "# input_sentence_1 = \"In the midst of a vibrant summer morning, with the sun casting its golden rays upon the lush green meadows and the fragrant wildflowers swaying gently in the warm breeze, a multitude of birds chirped melodiously while gracefully soaring across the clear blue sky, their wings glimmering like tiny diamonds as they embraced the boundless freedom of the open air, and nearby, a majestic oak tree stood tall and proud, its branches extending outward in a magnificent display of nature's artistry, providing shade and shelter for a variety of creatures that sought solace beneath its protective canopy, including a family of squirrels playfully darting between the branches, their bushy tails serving as vibrant accents against the backdrop of verdant leaves, and as the day progressed, the distant rumble of thunder gradually grew louder, heralding the imminent arrival of a summer storm, as dark clouds gathered overhead, casting an ephemeral gloom over the once vibrant landscape, yet even in the face of this impending tempest, there was an undeniable beauty in the contrast between the electric flashes of lightning that briefly illuminated the sky and the cascading raindrops that danced upon the earth, breathing life into the thirsty soil and rejuvenating the flora and fauna, and as the storm subsided, a mesmerizing rainbow emerged, arching gracefully across the horizon, its vibrant hues painting a breathtaking scene that filled hearts with awe and wonder, reminding us of the ever-present magic and resilience of nature, and in that fleeting moment, as the world basked in the afterglow of the storm, a profound sense of gratitude and harmony washed over everything, reminding us of our intricate connection to the vast tapestry of existence.\"\n",
        "# input_sentence_2 = \"The fundamental principles of calculus provide a powerful framework for understanding and analyzing the rates of change and accumulation of quantities in various fields of mathematics and science, enabling us to model and solve complex real-world problems with precision and rigor.\"\n",
        "# input_sentence_3 = \"I'm sorry for the misunderstanding, but as an AI developed by OpenAI, I don't have direct access to individual sentences or documents from my training data. I was trained on a mixture of licensed data, data created by human trainers, and publicly available data. These sources may contain a wide range of data, including books, websites, and other texts, so I don't have the ability to recall or generate any specific sentence from the training data. I generate responses based on patterns and information in the data I was trained on.\"\n",
        "# losses = optimize_for_neuron(input_sentence_3, neuron_index=2, layer_num=5)\n",
        "# # Plot losses\n",
        "# plt.figure(figsize=(10,6))\n",
        "# plt.plot([loss.cpu().detach() for loss in losses])\n",
        "# plt.title('Loss curve')\n",
        "# plt.xlabel('Iteration')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PMygm2yzt5Pz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def optimize_for_neuron_whole_input(neuron_index=0, layer_num=1, mlp_or_attention=\"mlp\", num_tokens=10, num_iterations=200):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      neuron_indices: List of indices.\n",
        "      mlp_or_attention (str): 'mlp' or 'attention'\n",
        "    \"\"\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n",
        "\n",
        "    # Start with random embeddings\n",
        "    embeddings = torch.randn((1, num_tokens, model.config.n_embd), device=device, requires_grad=True)\n",
        "\n",
        "    # Create an optimizer for the embeddings\n",
        "    optimizer = AdamW([embeddings], lr=0.1)  # You may need to adjust the learning rate\n",
        "\n",
        "    if 'mlp' in mlp_or_attention:\n",
        "        layer = model.transformer.h[layer_num].mlp\n",
        "    else:\n",
        "        raise NotImplementedError(\"Haven't implemented attention block yet\")\n",
        "\n",
        "    activation_saved = [torch.tensor(0.0, device=device)]\n",
        "    def hook(model, input, output):\n",
        "        # The output is a tensor. We're getting the average activation of the neuron across all tokens.\n",
        "        activation = output[0, :, neuron_index].mean()\n",
        "        activation_saved[0] = activation\n",
        "    handle = layer.register_forward_hook(hook)\n",
        "\n",
        "    pbar = tqdm(range(num_iterations), position=0, leave=True)\n",
        "    losses = []\n",
        "    for i in pbar:\n",
        "        # Construct input for the model using the embeddings directly\n",
        "        outputs = model(inputs_embeds=embeddings)\n",
        "        # We want to maximize activation, which is equivalent to minimizing negative activation\n",
        "        loss = -torch.sigmoid(activation_saved[0])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "        if i % (num_iterations//30) == 0:\n",
        "            pbar.set_description(f\"\"\"Loss at step {i}: {loss.item()}\\n\\n\n",
        "                                     Current sentence: {unembed_and_decode(embeddings)[0]}\\n\"\"\")\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    handle.remove()  # Don't forget to remove the hook!\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "scRH0RiR28VY"
      },
      "outputs": [],
      "source": [
        "# losses = optimize_for_neuron_whole_input(neuron_index=2, layer_num=5, num_tokens=20)\n",
        "# # Plot losses\n",
        "# plt.figure(figsize=(10,6))\n",
        "# plt.plot(losses)\n",
        "# plt.title('Loss curve')\n",
        "# plt.xlabel('Iteration')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.grid(True)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O2uMvZaTF7M-"
      },
      "outputs": [],
      "source": [
        "class Gpt2Autoencoder(torch.nn.Module):\n",
        "    def __init__(self, model_checkpoint, latent_dim=100, nhead=2, num_layers=6):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load the pretrained model\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
        "\n",
        "        # Create the encoder\n",
        "        self.encoder = copy.deepcopy(base_model)\n",
        "        self.encoder.lm_head = Linear(base_model.config.n_embd, latent_dim)\n",
        "\n",
        "        # Create the decoder from scratch\n",
        "        self.projection = Linear(latent_dim, base_model.config.n_embd)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=base_model.config.n_embd, nhead=nhead)\n",
        "        self.decoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers) # logits?\n",
        "\n",
        "    def forward(self, input_embeds, attention_mask=None):\n",
        "        # Encode the input\n",
        "        latent = self.encoder(inputs_embeds=input_embeds, attention_mask=attention_mask).logits\n",
        "\n",
        "        # Project the latent representation to the original embedding dimension\n",
        "        projected = self.projection(latent)\n",
        "\n",
        "        # Decode the projected representation\n",
        "        reconstructed_embeddings = self.decoder(projected)\n",
        "\n",
        "        return reconstructed_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gvd0cLx8YYQ",
        "outputId": "a5e4c034-29c9-41da-92b3-7c4994ffd5de"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Gpt2AutoencoderBoth(torch.nn.Module):\n",
        "    def __init__(self, model_checkpoint, latent_dim=100, nhead=2, num_layers=6):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load the pretrained model\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
        "        #self.encoder = nn.Linear(base_model.config.n_embd, latent_dim)\n",
        "        self.encoder = copy.deepcopy(base_model)\n",
        "        self.encoder.lm_head = Linear(base_model.config.n_embd, latent_dim)\n",
        "        self.projection = nn.Linear(latent_dim, base_model.config.n_embd)\n",
        "        self.decoder = Linear(base_model.config.n_embd, base_model.config.n_embd)\n",
        "        #self.decoder = copy.deepcopy(base_model) # what about positional embeddings?\n",
        "        #self.decoder.lm_head = Linear(base_model.config.n_embd, base_model.config.n_embd)\n",
        "\n",
        "    def forward(self, input_embeds, attention_mask=None):\n",
        "        latent = self.encoder(inputs_embeds=input_embeds, attention_mask=attention_mask).logits\n",
        "        #latent = self.encoder(input_embeds)\n",
        "        projected = self.projection(latent)\n",
        "        #return projected\n",
        "        reconstructed_embeddings = self.decoder(projected)\n",
        "        #reconstructed_embeddings = self.decoder(inputs_embeds = projected).logits\n",
        "        #assert reconstructed_embeddings.shape == input_embeds.shape, f\"shape must match of input and output, got {reconstructed_embeddings.shape} and {input_embeds.shape}\"\n",
        "        return reconstructed_embeddings"
      ],
      "metadata": {
        "id": "dG6TdUb_8HVk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence(model, tokenizer, max_length=50):\n",
        "    # Randomly select a token\n",
        "    random_token = random.randint(0, 50256)\n",
        "\n",
        "    # Convert the token to a tensor\n",
        "    input_ids = torch.tensor([[random_token]]).to(device)\n",
        "\n",
        "    # Generate a sequence of tokens\n",
        "    output = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=1.0, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    # Decode the tokens into a sentence\n",
        "    sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def get_sentence_similarity(sentence1, sentence2):\n",
        "\n",
        "    # Get embeddings for both sentences\n",
        "    response1 = openai.Embedding.create(input=sentence1, model=\"text-embedding-ada-002\")\n",
        "    response2 = openai.Embedding.create(input=sentence2, model=\"text-embedding-ada-002\")\n",
        "\n",
        "    embedding1 = np.array(response1['data'][0]['embedding'])\n",
        "    embedding2 = np.array(response2['data'][0]['embedding'])\n",
        "\n",
        "    # Compute cosine similarity between embeddings\n",
        "    similarity = cosine_similarity(embedding1.reshape(1, -1), embedding2.reshape(1, -1))\n",
        "\n",
        "    return similarity[0][0]"
      ],
      "metadata": {
        "id": "Dox4Gb1h0ZtH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_sentences():\n",
        "    pbar = tqdm(range(10))\n",
        "    sentences = []\n",
        "    # Generate random pairs of sentences\n",
        "    for i in pbar:\n",
        "        sentences.append(generate_sentence(model, tokenizer, max_length=50))\n",
        "    return sentences\n",
        "\n",
        "#sentences = gen_sentences()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9dd5f8787fb0480dae2ccac68cdd891d",
            "3928aba40e054634b1df3100872caf2b",
            "f1655bb2655e4efa9df464ef477e17c9",
            "0df2a3efd3044e97b7c8cde64eebe1d8",
            "44e524a2a088474e838b8f13715f22b3",
            "8aaeb85f86004699995fb9790f8e7e9d",
            "866bd3464a3f4efa8b6edbd842846823",
            "a574d91f3cb745feb109aef46cac488d",
            "68666bfefea240bb98c8b58b1c2b116e",
            "484592a3de554e6593e5d8fbaedc87af",
            "f05ba65c59294f4795959301decf1038"
          ]
        },
        "id": "KnPJL8RJmCDt",
        "outputId": "8483e812-b9e3-4f1c-ba55-6d544f46da78"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9dd5f8787fb0480dae2ccac68cdd891d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate random pairs of sentences using gpt2, and get the average ada embedding distance\n",
        "\n",
        "def calc_average_emb_distance(num_pairs):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
        "\n",
        "    # Initialize gpt2\n",
        "    model = AutoModelForCausalLM.from_pretrained('distilgpt2').to(device)\n",
        "    model.eval()\n",
        "\n",
        "    pbar = tqdm(range(num_pairs))\n",
        "    similarity_values = []\n",
        "    # Generate random pairs of sentences\n",
        "    for i in pbar:\n",
        "        sentence1 = generate_sentence(model, tokenizer, max_length=50)\n",
        "        sentence2 = generate_sentence(model, tokenizer, max_length=50)\n",
        "        # Compute cosine similarity between embeddings\n",
        "        similarity = get_sentence_similarity(sentence1, sentence2)\n",
        "        # Store the similarity value\n",
        "        similarity_values.append(similarity)\n",
        "        pbar.set_description(f\"Running average: {np.mean(similarity_values)}\")\n",
        "    # Print the average similarity value\n",
        "    return np.mean(similarity_values)\n",
        "\n",
        "# calc_average_emb_distance(10) #0.6956303872374302"
      ],
      "metadata": {
        "id": "JdqGb4lYuT8M"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss(sentence1, sentence2):\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    #padding='max_length', max_length=model.config.n_ctx\n",
        "    input_ids_1 = tokenizer.encode(sentence1, return_tensors=\"pt\", padding='longest').to(device)\n",
        "    embeddings_1 = model.transformer.wte(input_ids_1)\n",
        "    input_ids_2 = tokenizer.encode(sentence2, return_tensors=\"pt\", padding='longest').to(device)\n",
        "    embeddings_2 = model.transformer.wte(input_ids_2)\n",
        "    #code.interact(local=locals())\n",
        "    #print(\"embeddings_shape:\", embeddings_1.shape, embeddings_2.shape)\n",
        "    return torch.mean(torch.norm(embeddings_1 - embeddings_2, dim=2)).item()\n",
        "\n",
        "def train_autoencoder(autoencoder, criterion, optimizer, model, tokenizer, num_epochs=1000, print_every=1, save_path=\"transformer-autoencoder.pt\", load_path=None):\n",
        "    loss_values = []\n",
        "    similarity_values = []\n",
        "    constructed_losses = []\n",
        "\n",
        "    # Load the model's parameters from a checkpoint if provided\n",
        "    if load_path is not None:\n",
        "        autoencoder.load_state_dict(torch.load(load_path))\n",
        "\n",
        "    pbar = tqdm(range(num_epochs))\n",
        "    # Training loop\n",
        "    for epoch in pbar:\n",
        "        # Generate a sentence with the pretrained model\n",
        "        #input_sentence = generate_sentence(model, tokenizer, max_length=50)\n",
        "        input_sentence = sentences[0]\n",
        "        # Prepare the inputs for the autoencoder\n",
        "        # , max_length=model.config.n_ctx\n",
        "        input_ids = tokenizer.encode(input_sentence, return_tensors=\"pt\", padding='longest').to(device)\n",
        "        original_embeddings = model.transformer.wte(input_ids)\n",
        "        #print(\"orig:\", original_embeddings.shape)\n",
        "        # Run the autoencoder and compute the loss\n",
        "        #with autocast():\n",
        "        reconstructed_embeddings = autoencoder(original_embeddings)\n",
        "        loss = torch.mean(torch.norm(original_embeddings-reconstructed_embeddings, dim = 2))\n",
        "        #loss = criterion(reconstructed_embeddings, original_embeddings)\n",
        "        #code.interact(local=locals())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Record the loss value for plotting\n",
        "        loss_values.append(loss.item())\n",
        "\n",
        "        #print(\"reconstructed embs:\", reconstructed_embeddings.shape)\n",
        "        # Compute the sentence similarity between the original and reconstructed sentences\n",
        "        reconstructed_sentence = unembed_and_decode(reconstructed_embeddings)[0]\n",
        "        # reconstructed_sentence has too many tokens?\n",
        "        #print(\"recon:\", len(reconstructed_sentence))\n",
        "        # Print progress and save model every 'print_every' epochs\n",
        "        if epoch % print_every == 0:\n",
        "            similarity = get_sentence_similarity(input_sentence, reconstructed_sentence)\n",
        "            similarity_values.append(similarity)\n",
        "            #constructed_loss = calc_loss(input_sentence, reconstructed_sentence)\n",
        "            #constructed_losses.append(constructed_loss)\n",
        "            pbar.set_description(\n",
        "                f\"\"\"Original: {input_sentence}\n",
        "                Reconstructed: {reconstructed_sentence}\n",
        "                Constructed Loss: constructed_loss\n",
        "                Epoch {epoch}/{num_epochs}, Loss: {loss.item()}, Similarity: {similarity}\"\"\")\n",
        "            torch.save(autoencoder.state_dict(), save_path)\n",
        "\n",
        "    # Plot the loss values, similarity values, and constructed losses\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Training Step')\n",
        "    ax1.set_ylabel('Loss', color=color)\n",
        "    ax1.plot(loss_values, color=color)\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "    ax1.set_ylim(0, 5)\n",
        "\n",
        "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Similarity', color=color)  # we already handled the x-label with ax1\n",
        "    ax2.plot(range(0, num_epochs, print_every), similarity_values, color=color)\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    ## Add constructed losses to the plot\n",
        "    #ax3 = ax1.twinx()  # instantiate a third axes that shares the same x-axis\n",
        "    #ax3.spines['right'].set_position(('outward', 60))  # move the y-axis\n",
        "    #color = 'tab:green'\n",
        "    #ax3.set_ylabel('Constructed Losses', color=color)\n",
        "    #ax3.plot(range(0, num_epochs, print_every), constructed_losses, color=color)\n",
        "    #ax3.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "w_5CuWTSrUsk"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_loss(\"hello\",\"there\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3Yv787evqds",
        "outputId": "07fa481b-e5d5-4680-c8af-6612c1fa16eb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.601763725280762"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694,
          "referenced_widgets": [
            "d83cfc6ee91b4941bd8976fe544fbe3a",
            "4178c9130e264dfd8d29dac57e66cede",
            "2fb0f94bfbad4d1fbc78726eb34f3dbc",
            "8b0ae37f3b0946fb99b31dbd703f5743",
            "f14e4242b0474a0fbd13935e07747910",
            "ee49409f322d4a1399b64f8e638ff276",
            "f48805775c324d1194802734aa9e5d69",
            "5e8fba85f48d4ac0811ba7f60d2cd288",
            "d4d19604b581404f9ceb920b577f0de4",
            "b334a6a505f24c01a8b8402a9bdbfa19",
            "b624095f2a454489ac30c648aea80427"
          ]
        },
        "id": "EfN8JUtdDrea",
        "outputId": "2c06b56d-1142-47fd-c131-68c6c805bc87"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d83cfc6ee91b4941bd8976fe544fbe3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# Initialize autoencoder\n",
        "autoencoder = Gpt2AutoencoderBoth('distilgpt2').to(device)\n",
        "\n",
        "# Initialize loss function, optimizer, and gradient scaler for mixed-precision training\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(autoencoder.parameters(), lr=.1)\n",
        "\n",
        "# # watch the weights\n",
        "# watcher = ww.WeightWatcher(model=autoencoder)\n",
        "# details = watcher.analyze(plot=False)\n",
        "# Call the training function without a path to a checkpoint\n",
        "train_autoencoder(autoencoder, criterion, optimizer, model, tokenizer, num_epochs=3000, print_every=100) #load_path ="
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "details2 = watcher.analyze(plot=True) # warning, this prints a lot of plots and takes forever"
      ],
      "metadata": {
        "id": "mZdGTNmAZazH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_autoencoder(autoencoder, model, tokenizer, num_samples=10, print_every=3):\n",
        "    # Prepare the autoencoder for evaluation\n",
        "    # autoencoder.eval() # <- do we need to implement this?\n",
        "\n",
        "    similarities = []\n",
        "\n",
        "    for i in tqdm(range(num_samples)):\n",
        "        # Generate a sentence with the model\n",
        "        input_sentence = generate_sentence(model, tokenizer, max_length=50)\n",
        "\n",
        "        # Encode the sentence\n",
        "        input_ids = tokenizer.encode(input_sentence, return_tensors=\"pt\").to(device)\n",
        "        original_embeddings = model.transformer.wte(input_ids)\n",
        "\n",
        "        # Pass the encoded sentence through the autoencoder\n",
        "        with torch.no_grad():\n",
        "            reconstructed_embeddings = autoencoder(original_embeddings)\n",
        "\n",
        "        # Decode the output of the autoencoder\n",
        "        reconstructed_sentence = unembed_and_decode(reconstructed_embeddings)\n",
        "\n",
        "        # Compute the sentence similarity between the original and reconstructed sentences\n",
        "        similarity = get_sentence_similarity(input_sentence, reconstructed_sentence)\n",
        "        similarities.append(similarity)\n",
        "\n",
        "        if i % print_every==0:\n",
        "          print(f\"Original sentence: {input_sentence}\")\n",
        "          print(f\"Reconstructed sentence: {reconstructed_sentence}\")\n",
        "          print(f\"Similarity: {similarity}\")\n",
        "          print()\n",
        "\n",
        "    # Compute the average sentence similarity\n",
        "    average_similarity = np.mean(similarities)\n",
        "\n",
        "    print(f\"Average sentence similarity between original and reconstructed sentences: {average_similarity}\")\n"
      ],
      "metadata": {
        "id": "pOsJP8pklgu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
        "\n",
        "# Initialize autoencoder\n",
        "autoencoder = Gpt2Autoencoder('distilgpt2').to(device)\n",
        "autoencoder.load_state_dict(torch.load(\"transformer-autoencoder.pt\"))\n",
        "\n",
        "# Initialize loss function, optimizer, and gradient scaler for mixed-precision training\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(autoencoder.parameters())\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Call the training function without a path to a checkpoint\n",
        "evaluate_autoencoder(autoencoder, model, tokenizer) #load_path ="
      ],
      "metadata": {
        "id": "i_LMQoJ5ljxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDnci_DnWaOV"
      },
      "outputs": [],
      "source": [
        "def optimize_for_neuron_whole_input(neuron_index=0, layer_num=1, mlp_or_attention=\"mlp\", num_tokens=10, num_iterations=200):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      neuron_indices: List of indices.\n",
        "      mlp_or_attention (str): 'mlp' or 'attention'\n",
        "    \"\"\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n",
        "    autoencoder = Gpt2Autoencoder('distilgpt2').to(device)\n",
        "    autoencoder.load_state_dict(torch.load('transformer-autoencoder.pt'))\n",
        "\n",
        "    # Get the dimensionality of the latent space\n",
        "    latent_dim = autoencoder.encoder.lm_head.weight.shape[0]\n",
        "\n",
        "    # Start with random latent vectors\n",
        "    latent_vectors = torch.randn((1, num_tokens, latent_dim), device=device, requires_grad=True)\n",
        "\n",
        "    # Create an optimizer for the latent vectors\n",
        "    optimizer = AdamW([latent_vectors], lr=0.1)  # You may need to adjust the learning rate\n",
        "\n",
        "    if 'mlp' in mlp_or_attention:\n",
        "        layer = model.transformer.h[layer_num].mlp.c_fc\n",
        "    elif 'attention' in mlp_or_attention:\n",
        "        layer = model.transformer.h[layer_num].attn.c_attn\n",
        "    else:\n",
        "        raise NotImplementedError(\"Haven't implemented attention block yet\")\n",
        "\n",
        "    activation_saved = [torch.tensor(0.0, device=device)]\n",
        "    def hook(model, input, output):\n",
        "        # The output is a tensor. We're getting the average activation of the neuron across all tokens.\n",
        "        activation = output[0, :, neuron_index].mean()\n",
        "        activation_saved[0] = activation\n",
        "    handle = layer.register_forward_hook(hook)\n",
        "\n",
        "    losses = []\n",
        "    for i in tqdm(range(num_iterations), position=0, leave=True):\n",
        "        # Construct input for the model using the embeddings directly\n",
        "        embeddings = autoencoder.decoder(autoencoder.projection(latent_vectors))\n",
        "        outputs = model(inputs_embeds=embeddings)\n",
        "        # We want to maximize activation, which is equivalent to minimizing negative activation\n",
        "        loss = -torch.sigmoid(activation_saved[0])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "        if i % (num_iterations//30) == 0:\n",
        "            tqdm.write(f\"Loss at step {i}: {loss.item()}\\n\", end='')\n",
        "            tqdm.write(unembed_and_decode(embeddings)[0], end='')\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    handle.remove()  # Don't forget to remove the hook!\n",
        "    return losses\n",
        "\n",
        "losses = optimize_for_neuron_whole_input(neuron_index=2, layer_num=5, num_tokens=20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder"
      ],
      "metadata": {
        "id": "6NRzXJK5nijX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fc48JiFxnoro"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9dd5f8787fb0480dae2ccac68cdd891d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3928aba40e054634b1df3100872caf2b",
              "IPY_MODEL_f1655bb2655e4efa9df464ef477e17c9",
              "IPY_MODEL_0df2a3efd3044e97b7c8cde64eebe1d8"
            ],
            "layout": "IPY_MODEL_44e524a2a088474e838b8f13715f22b3"
          }
        },
        "3928aba40e054634b1df3100872caf2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aaeb85f86004699995fb9790f8e7e9d",
            "placeholder": "​",
            "style": "IPY_MODEL_866bd3464a3f4efa8b6edbd842846823",
            "value": "100%"
          }
        },
        "f1655bb2655e4efa9df464ef477e17c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a574d91f3cb745feb109aef46cac488d",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68666bfefea240bb98c8b58b1c2b116e",
            "value": 10
          }
        },
        "0df2a3efd3044e97b7c8cde64eebe1d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_484592a3de554e6593e5d8fbaedc87af",
            "placeholder": "​",
            "style": "IPY_MODEL_f05ba65c59294f4795959301decf1038",
            "value": " 10/10 [00:12&lt;00:00,  1.86it/s]"
          }
        },
        "44e524a2a088474e838b8f13715f22b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aaeb85f86004699995fb9790f8e7e9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "866bd3464a3f4efa8b6edbd842846823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a574d91f3cb745feb109aef46cac488d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68666bfefea240bb98c8b58b1c2b116e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "484592a3de554e6593e5d8fbaedc87af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f05ba65c59294f4795959301decf1038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d83cfc6ee91b4941bd8976fe544fbe3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4178c9130e264dfd8d29dac57e66cede",
              "IPY_MODEL_2fb0f94bfbad4d1fbc78726eb34f3dbc",
              "IPY_MODEL_8b0ae37f3b0946fb99b31dbd703f5743"
            ],
            "layout": "IPY_MODEL_f14e4242b0474a0fbd13935e07747910"
          }
        },
        "4178c9130e264dfd8d29dac57e66cede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee49409f322d4a1399b64f8e638ff276",
            "placeholder": "​",
            "style": "IPY_MODEL_f48805775c324d1194802734aa9e5d69",
            "value": "Original: PersonSolo.co.uk or call 0144 574 638\n                Reconstructed:  mathemat mathemat mathemat mathemat mathemat mathemat mathemat mathemat mathemat mathemat mathemat mathemat mathemat mathemat mathemat\n                Constructed Loss: constructed_loss \n                Epoch 2900/3000, Loss: 2.8340163230895996, Similarity: 0.7071600206114944: 100%"
          }
        },
        "2fb0f94bfbad4d1fbc78726eb34f3dbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e8fba85f48d4ac0811ba7f60d2cd288",
            "max": 3000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4d19604b581404f9ceb920b577f0de4",
            "value": 3000
          }
        },
        "8b0ae37f3b0946fb99b31dbd703f5743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b334a6a505f24c01a8b8402a9bdbfa19",
            "placeholder": "​",
            "style": "IPY_MODEL_b624095f2a454489ac30c648aea80427",
            "value": " 3000/3000 [03:02&lt;00:00, 21.54it/s]"
          }
        },
        "f14e4242b0474a0fbd13935e07747910": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee49409f322d4a1399b64f8e638ff276": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f48805775c324d1194802734aa9e5d69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e8fba85f48d4ac0811ba7f60d2cd288": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4d19604b581404f9ceb920b577f0de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b334a6a505f24c01a8b8402a9bdbfa19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b624095f2a454489ac30c648aea80427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}